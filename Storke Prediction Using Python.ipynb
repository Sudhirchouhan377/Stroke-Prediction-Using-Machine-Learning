import pandas as pd

dF = pd.read_csv(r'/content/healthcare-dataset-stroke-data1.csv')

dF.head()

dF.isnull().sum()

nan_bmi_stroke = dF[dF['bmi'].isna() & (dF['stroke'] == 1)].shape[0]
non_nan_bmi_stroke = dF[dF['bmi'].notna() & (dF['stroke'] == 1)].shape[0]

print(f"Number of stroke cases with NaN BMI: {nan_bmi_stroke}")
print(f"Number of stroke cases with non-NaN BMI: {non_nan_bmi_stroke}")

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer

# Plot before imputation
plt.figure(figsize=(8, 4))
sns.boxplot(x=dF['bmi'])
plt.title('BMI Before Imputation')
plt.tight_layout()
plt.show()

# Variance before imputation
print("Variance before imputation:", dF['bmi'].var())

# Imputation using median
imputer = SimpleImputer(strategy='median')
dF['bmi'] = imputer.fit_transform(dF[['bmi']])

# Plot after imputation
plt.figure(figsize=(8, 4))
sns.boxplot(x=dF['bmi'])
plt.title('BMI After Imputation')
plt.tight_layout()
plt.show()

# Variance after imputation
print("Variance after imputation:", dF['bmi'].var())


import seaborn as sns
import matplotlib.pyplot as plt

def cap_outliers_iqr(df, column):
    Q1 = dF[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    capped = dF[column].clip(lower, upper)
    return capped, lower, upper

# Apply IQR capping to 'bmi'
dF['bmi_capped'], bmi_lower, bmi_upper = cap_outliers_iqr(dF, 'bmi')

# Plot original and capped BMI side by side
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

sns.boxplot(x=dF['bmi'], ax=axes[0]).set_title('BMI - Original')
sns.boxplot(x=dF['bmi_capped'], ax=axes[1]).set_title('BMI - Capped')

plt.tight_layout()
plt.show()

dF.head(10)

dF.isnull().sum()

# Categorical Feature Vizuliazation

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

import seaborn as sns
sns.set_theme(style="whitegrid")


sns.set_palette("Set2")
plt.figure(figsize=(5, 5))
dF['gender'].value_counts().plot.pie(autopct='%1.1f%%', startangle=90, colors=plt.cm.Pastel1.colors, wedgeprops={'edgecolor': 'black', 'linewidth': 1.5})
plt.title('Gender Distribution')
plt.ylabel('')
plt.show()


dF['gender'].value_counts()

dF['gender'] = dF['gender'].replace('Other','Female')
dF['gender'].value_counts()

import matplotlib.pyplot as plt

married_counts = dF['ever_married'].value_counts()

plt.figure(figsize=(6, 6))
plt.pie(married_counts, labels=married_counts.index, autopct='%1.1f%%', startangle=90, colors=plt.cm.Pastel1.colors, wedgeprops={'edgecolor': 'black', 'linewidth': 1.5})
plt.title('Marriage Status Distribution')
plt.axis('equal')
plt.show()



plt.figure(figsize=(7, 4))
sns.countplot(x='work_type', data=dF,edgecolor = 'black' ,linewidth = 1.5)
plt.title('Work Type Distribution')
plt.xlabel('Work Type')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()


res_counts = dF['Residence_type'].value_counts()
plt.figure(figsize=(5, 5))
plt.pie(res_counts, labels=res_counts.index, autopct='%1.1f%%', startangle=90, wedgeprops=dict(width=0.4, edgecolor = 'black'))
plt.title('Residence Type (Urban vs Rural)')
plt.show()

plt.figure(figsize=(6, 4))
dF['smoking_status'].value_counts().plot(kind='barh', color='teal',edgecolor = 'black', linewidth = 1.5)
plt.title('Smoking Status')
plt.xlabel('Count')
plt.ylabel('Smoking Category')
plt.show()

# Encoding Feature

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder

cat_cols = ['work_type', 'smoking_status']
label_cols = ['gender', 'Residence_type', 'ever_married']

dF_encoded_first = pd.get_dummies(dF, columns=cat_cols, drop_first=True, dtype=int)

le = LabelEncoder()
dF_label_encoded = dF[label_cols].apply(le.fit_transform)

dF_encoded = pd.concat([dF_encoded_first.drop(columns=label_cols), dF_label_encoded], axis=1)


dF_encoded

# Normalization

from sklearn.preprocessing import MinMaxScaler

columns_to_scale = ['age','bmi','avg_glucose_level']
scaler = MinMaxScaler()

scaler.fit(dF_encoded[columns_to_scale])
columns_scaled=scaler.transform(dF_encoded[columns_to_scale])

columns_scaled=pd.DataFrame(columns_scaled, columns=columns_to_scale)

print("Original Data Description:")
np.round(dF_encoded[columns_to_scale].describe(), 1)

print("Scaled Data Description:")
np.round(columns_scaled.describe(),1)

dF_encoded[columns_to_scale] = columns_scaled

dF_encoded.head(10)


fig, axs = plt.subplots(2, 3, figsize=(15, 8))

for i, col in enumerate(columns_to_scale):

    axs[0, i].hist(dF[col], bins=20, color='skyblue', edgecolor='black')
    axs[0, i].set_title(f'Original {col}')
    axs[0, i].set_xlabel(col)


    axs[1, i].hist(columns_scaled[col], bins=20, color='salmon', edgecolor='black')
    axs[1, i].set_title(f'Scaled {col}')
    axs[1, i].set_xlabel(f'Scaled {col}')

axs[0, 0].set_ylabel("Frequency")
axs[1, 0].set_ylabel("Frequency")
plt.tight_layout()
plt.show()

#Correlation

dF_new = dF_encoded.drop(columns=['id'])

corr = dF_new.select_dtypes(include='number').corr()

plt.figure(figsize=(12, 10))
sns.heatmap(corr, annot=True, fmt=".2f", square=True)
plt.title("Correlation Matrix")

plt.show()

Correlation = corr['stroke'].drop('stroke')

Correlation

# Feature Selection

dF_encoded.drop('id', axis=1, inplace=True)

X = dF_encoded.drop('stroke', axis=1)
y = dF_encoded['stroke']


from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestClassifier
estimator = RandomForestClassifier(random_state=42)
selector = RFE(estimator, n_features_to_select=7)
selector.fit(X, y)

selected_features = X.columns[selector.support_]

top_feature = X[selected_features]
top_feature

import matplotlib.pyplot as plt


estimator.fit(top_feature, y)

importances = estimator.feature_importances_

plt.figure(figsize=(8, 6))
plt.barh(selected_features, importances, color='Skyblue', edgecolor='black', linewidth=1.5)
plt.xlabel("Feature Importance Score")
plt.title("Top 7 Features Selected Using RFE (Random Forest)")
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()


# Checking Biasness

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

X = top_feature
y = dF['stroke']

from sklearn.model_selection import train_test_split
X_train_, X_test_, y_train_, y_test_ = train_test_split(X, y, test_size=0.3, random_state=42)

model = RandomForestClassifier(random_state=42)
model.fit(X_train_, y_train_)
y_pred_ = model.predict(X_test_)

print("Classification Report:\n",classification_report(y_test_, y_pred_))
print("Confusion Matrix:\n",confusion_matrix(y_test_, y_pred_))

import seaborn as sns
import matplotlib.pyplot as plt

dF['stroke'].value_counts(normalize = True).plot(kind = 'bar', color = ['pink','black'])
plt.title("Stroke Class Distribution")
plt.xlabel("Stroke ")
plt.ylabel("Count")
plt.show()
print(dF['stroke'].value_counts())

# **balancing model**

from imblearn.under_sampling import RandomUnderSampler
from sklearn.ensemble import ExtraTreesClassifier

rus = RandomUnderSampler(random_state=42)
X_res, y_res = rus.fit_resample(X, y)

model_ = ExtraTreesClassifier(random_state=42)
model_.fit(X_res, y_res)

import matplotlib.pyplot as plt
from collections import Counter


before_counts = Counter(y)

after_counts = Counter(y_res)

balance_df = pd.DataFrame({
    'Before Resampling': pd.Series(before_counts),
    'After Resampling': pd.Series(after_counts)
})

balance_df.plot(kind='bar', figsize=(8,5), color=['skyblue', 'black'])
plt.title('Class Distribution Before and After RandomUnderSampler')
plt.xlabel('Class')
plt.ylabel('Number of Samples')
plt.xticks(rotation=0)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()


print('x train and y train resampled shapes: ', X_res.shape, y_res.shape)

print('number of stroke data after undersampling: ',len(X_res[y_res==1]))
print('number of non stroke data after undersampling: ',len(X_res[y_res==0]))

from sklearn.model_selection import train_test_split
X_train, X_test,y_train, y_test =  train_test_split(X_res,y_res,test_size=0.2,random_state = 42)

y_pre_res = model_.predict(X_test)

print(classification_report(y_test, y_pre_res))

print('x train and y train shape: ', X_train.shape, y_train.shape)
print('x test and y test shape: ',X_test.shape, y_test.shape)

# Hyper parameter tunning

import warnings
warnings.filterwarnings("ignore")

import numpy as np
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report

from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

models = {
    'RandomForest': {
        'model': RandomForestClassifier(random_state=42),
        'params': {
            'n_estimators': [100, 200],
            'max_depth': [5, 10, None],
            'min_samples_split': [2, 5]
        }
    }
}


for name, mp in models.items():
    print(f"\n=== {name} ===")

    grid = GridSearchCV(
        estimator=mp['model'],
        param_grid=mp['params'],
        scoring='f1',
        cv=5,
        n_jobs=-1,
        verbose=1
    )

    grid.fit(X_train, y_train)

    rf_best = grid.best_estimator_
    print("Best Parameters:", grid.best_params_)

    y_pred = rf_best.predict(X_test)
    print("Classification Report:\n", classification_report(y_test, y_pred))

# **Ensemble technique**

## Stacking



import warnings
warnings.filterwarnings("ignore")
import numpy as np

from sklearn.ensemble import StackingClassifier, BaggingClassifier, GradientBoostingClassifier, RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score

bagging_model = BaggingClassifier(
    estimator=DecisionTreeClassifier(max_depth=5),
    n_estimators=50,
    random_state=42
)


gb_model = GradientBoostingClassifier(
    n_estimators=100,
    learning_rate=0.05,
    max_depth=3,
    random_state=42
)


meta_learner = LogisticRegression(
    max_iter=1000,
    random_state=42,
    class_weight='balanced'
)


base_learners = [
    ('rf', rf_best),
    ('bag', bagging_model),
    ('boost', gb_model)
]


stacked_model = StackingClassifier(
    estimators=base_learners,
    final_estimator=meta_learner,
    cv=5,
    passthrough=True,
    n_jobs=-1
)


stacked_model.fit(X_train, y_train)


y_pred_stack = stacked_model.predict(X_test)
print("Classification Report:\n",classification_report(y_test, y_pred_stack))

# **Overfitting check**

train_preds = stacked_model.predict(X_train)
test_preds = stacked_model.predict(X_test)

from sklearn.metrics import accuracy_score, f1_score

print("Train Accuracy:", accuracy_score(y_train, train_preds))
print("Test Accuracy:", accuracy_score(y_test, test_preds))

print("Train F1 Score:", f1_score(y_train, train_preds))
print("Test F1 Score:", f1_score(y_test, test_preds))

# **ROC AUC Curve**

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

stacked_probs = stacked_model.predict_proba(X_test)[:, 1]

rand_probs = [0 for _ in range(len(y_test))]

rand_auc = roc_auc_score(y_test, rand_probs)
stacked_auc = roc_auc_score(y_test, stacked_probs)

print('Random prediction AUROC =', rand_auc)
print('Stacked Model AUROC =', stacked_auc)

rand_fpr, rand_tpr, _ = roc_curve(y_test, rand_probs)
stacked_fpr, stacked_tpr, _ = roc_curve(y_test, stacked_probs)

plt.figure(figsize=(8, 6))
plt.plot(rand_fpr, rand_tpr, linestyle='--', color='black', label='Random prediction - AUROC = %0.2f' % rand_auc)
plt.plot(stacked_fpr, stacked_tpr, color='red', label='Stacked Model - AUROC = %0.3f' % stacked_auc)
plt.title('ROC Curve - Stacked Model vs Random')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

import joblib

model_bundle = {
    'model': stacked_model,
    'columns': X_train.columns.tolist(),
    'scaler': scaler
}

joblib.dump(model_bundle, 'model.joblib')

print("Model saved as model.joblib")
